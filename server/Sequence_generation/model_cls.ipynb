{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "058e0175-ef36-46b5-b2b1-1d35d258311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "SEQ_LEN = 6                 \n",
    "HORIZONS = 3               \n",
    "PATCH_SIZE = 13             \n",
    "HALF = PATCH_SIZE // 2\n",
    "FILL_NAN_VALUE = 0.0\n",
    "\n",
    "REQUIRED_COLS = [\n",
    "    \"era5_t2m_file\", \"era5_d2m_file\", \"era5_tp_file\",\n",
    "    \"era5_u10_file\", \"era5_v10_file\",\n",
    "    \"viirs_file\", \"dem_file\", \"lulc_file\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed507d4-d0c0-4c23-b3f4-e12115f05a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5774f7-5a3f-4c9b-b70b-bfdfafab3ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ae142a5-292c-4193-9e60-d501771b46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_single_raster(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        arr = src.read() \n",
    "\n",
    "    if arr.shape[0] == 1:\n",
    "        \n",
    "        return arr[0]\n",
    "    else:\n",
    "     \n",
    "        return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d95c89-2d72-4f83-897d-966405060ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rasters(df, raster_cols, max_workers=8):\n",
    "    \n",
    "    all_paths = set()\n",
    "\n",
    "    for col in raster_cols:\n",
    "        if col in df.columns:\n",
    "            all_paths.update(df[col].dropna().unique())\n",
    "    all_paths = list(all_paths)\n",
    "\n",
    "    cache = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        results = list(ex.map(_load_single_raster, all_paths))\n",
    "\n",
    "    for path, arr in zip(all_paths, results):\n",
    "        if arr is not None:\n",
    "            cache[path] = arr\n",
    "    return cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edee5feb-0f88-4090-ab3f-4c28838c8f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_center(h, w, patch_size=PATCH_SIZE):\n",
    "    half = patch_size // 2\n",
    "    r = np.clip(h // 2, half, h - half - 1)\n",
    "    c = np.clip(w // 2, half, w - half - 1)\n",
    "    return r, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fce6a62-85f9-4b91-a739-a40022b4718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_patch(arr, row, col, patch_size=PATCH_SIZE):\n",
    "    half = patch_size // 2\n",
    "    h, w = arr.shape\n",
    "\n",
    "    r0 = row - half\n",
    "    r1 = row + half + 1\n",
    "    c0 = col - half\n",
    "    c1 = col + half + 1\n",
    "\n",
    "    patch = np.zeros((patch_size, patch_size), dtype=arr.dtype)\n",
    "\n",
    "    r0_clip = max(r0, 0)\n",
    "    r1_clip = min(r1, h)\n",
    "    c0_clip = max(c0, 0)\n",
    "    c1_clip = min(c1, w)\n",
    "\n",
    "    pr0 = r0_clip - r0\n",
    "    pr1 = pr0 + (r1_clip - r0_clip)\n",
    "    pc0 = c0_clip - c0\n",
    "    pc1 = pc0 + (c1_clip - c0_clip)\n",
    "\n",
    "    patch[pr0:pr1, pc0:pc1] = arr[r0_clip:r1_clip, c0_clip:c1_clip]\n",
    "\n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e402e30a-b656-4fe1-9eb9-99fc3e3171a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sample(seq_rows, horizon_rows, cache, force_fire=False):\n",
    "    seq_patches = []\n",
    "\n",
    "   \n",
    "    for _, row in seq_rows.iterrows():\n",
    "        bands = []\n",
    "        for var in [\"era5_t2m_file\", \"era5_d2m_file\", \"era5_tp_file\",\n",
    "                    \"era5_u10_file\", \"era5_v10_file\"]:\n",
    "            arr = cache[row[var]]\n",
    "\n",
    "            if len(arr.shape) == 3:\n",
    "                arr = arr[0]\n",
    "\n",
    "            h, w = arr.shape\n",
    "            r, c = _safe_center(h, w)\n",
    "            bands.append(_extract_patch(arr, r, c))\n",
    "\n",
    "        dem = cache[row[\"dem_file\"]]\n",
    "        lulc = cache[row[\"lulc_file\"]]\n",
    "\n",
    "        if len(dem.shape) == 3:\n",
    "            dem = dem[0]\n",
    "        if len(lulc.shape) == 3:\n",
    "            lulc = lulc[0]\n",
    "\n",
    "        h, w = dem.shape\n",
    "        r, c = _safe_center(h, w)\n",
    "        bands.append(_extract_patch(dem, r, c))\n",
    "        bands.append(_extract_patch(lulc, r, c))\n",
    "\n",
    "        seq_patches.append(np.stack(bands, axis=-1))\n",
    "\n",
    "    X = np.stack(seq_patches, axis=0)\n",
    "\n",
    "\n",
    "    horizon_patches = []\n",
    "   \n",
    "    for _, row in horizon_rows.iterrows():\n",
    "        viirs_stack = cache[row[\"viirs_file\"]]\n",
    "        \n",
    "    \n",
    "        target_band_idx_list = eval(row[\"target_band_idxs\"])\n",
    " \n",
    "        idx = target_band_idx_list[0]\n",
    "        \n",
    "        band = viirs_stack[idx - 1]\n",
    "        h, w = band.shape\n",
    "        r, c = _safe_center(h, w)\n",
    "\n",
    "        if force_fire and np.any(band > 0):\n",
    "            fire_pos = np.argwhere(band > 0)\n",
    "            r, c = fire_pos[np.random.randint(len(fire_pos))]\n",
    "\n",
    "        horizon_patches.append(_extract_patch(band, r, c))\n",
    "\n",
    "    y = np.stack(horizon_patches, axis=0)\n",
    "\n",
    "    return X.astype(\"float32\"), y.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32da5b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(df, cache, fire_ratio=0.5):\n",
    "    valid_start_indices = list(range(len(df) - SEQ_LEN - HORIZONS + 1))\n",
    "    fire_start_indices = []\n",
    "    non_fire_start_indices = []\n",
    "    \n",
    "    print(\"Scanning data for fire and non-fire events...\")\n",
    "    for i in valid_start_indices:\n",
    "        horizon_rows = df.iloc[i + SEQ_LEN : i + SEQ_LEN + HORIZONS]\n",
    "        has_fire = any(np.any(cache[row[\"viirs_file\"]] > 0) for _, row in horizon_rows.iterrows())\n",
    "        \n",
    "        if has_fire:\n",
    "            fire_start_indices.append(i)\n",
    "        else:\n",
    "            non_fire_start_indices.append(i)\n",
    "\n",
    "    num_fire_samples = len(fire_start_indices)\n",
    "    \n",
    "    if num_fire_samples == 0:\n",
    "        print(\"Warning: No fire events found in the dataset.\")\n",
    "        num_non_fire_samples_to_use = min(len(non_fire_start_indices), 1000) \n",
    "    else:\n",
    "        num_non_fire_samples_to_use = int((num_fire_samples / fire_ratio) - num_fire_samples)\n",
    "        num_non_fire_samples_to_use = min(num_non_fire_samples_to_use, len(non_fire_start_indices))\n",
    "\n",
    "    fire_indices_to_use = fire_start_indices\n",
    "\n",
    "    if len(non_fire_start_indices) > 0 and num_non_fire_samples_to_use > 0:\n",
    "      non_fire_indices_to_use = np.random.choice(\n",
    "          non_fire_start_indices,\n",
    "          size=num_non_fire_samples_to_use,\n",
    "          replace=False \n",
    "      )\n",
    "      indices_to_use = np.concatenate([fire_indices_to_use, non_fire_indices_to_use])\n",
    "    else:\n",
    "      indices_to_use = np.array(fire_indices_to_use)\n",
    "\n",
    "    np.random.shuffle(indices_to_use)\n",
    "    indices_to_use = indices_to_use.astype(int)\n",
    "    \n",
    "    print(f\"Generator initialized. Found {len(fire_indices_to_use)} fire samples and using {len(indices_to_use) - len(fire_indices_to_use)} non-fire samples.\")\n",
    "\n",
    "    for i in indices_to_use:\n",
    "        seq_rows = df.iloc[i : i + SEQ_LEN]\n",
    "        horizon_rows = df.iloc[i + SEQ_LEN : i + SEQ_LEN + HORIZONS]\n",
    "        X, y = build_sample(seq_rows, horizon_rows, cache)\n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbccc812-c0bf-4446-9bbe-8e352a0c2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_dataset(df, cache, shuffle_buf=256):\n",
    "#     output_signature = (\n",
    "#         tf.TensorSpec(shape=(SEQ_LEN, PATCH_SIZE, PATCH_SIZE, 7), dtype=tf.float32),\n",
    "#         tf.TensorSpec(shape=(HORIZONS, PATCH_SIZE, PATCH_SIZE), dtype=tf.float32),\n",
    "#     )\n",
    "    \n",
    "#     ds = tf.data.Dataset.from_generator(\n",
    "#         lambda: make_generator(df, cache),\n",
    "#         output_signature=output_signature\n",
    "#     )\n",
    "    \n",
    "#     ds = ds.shuffle(shuffle_buf, reshuffle_each_iteration=True)\n",
    "#     ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "#     return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d67068e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, cache, shuffle=True, ensure_fire=True, shuffle_buf=256):\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=(SEQ_LEN, PATCH_SIZE, PATCH_SIZE, 7), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(HORIZONS, PATCH_SIZE, PATCH_SIZE), dtype=tf.float32),\n",
    "    )\n",
    "    \n",
    "    # CORRECTED LINE:\n",
    "    # Changed the keyword argument from 'ensure_fire=' to 'fire_ratio='\n",
    "    # This now correctly passes the value to the make_generator function.\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: make_generator(df, cache, fire_ratio=ensure_fire),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_buf, reshuffle_each_iteration=True)\n",
    "    \n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e327f91e-e454-4b6c-8ea4-f6b8b6aae545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 17535\n",
      "Train samples: 14028\n",
      "Validation samples: 3507\n",
      "Loading rasters into memory...\n",
      "Loaded 9 rasters into memory ✅\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    csv_path = r\"C:\\Users\\Ankit\\Datasets_Forest_fire\\sequence_index_hourly_binary.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    TOTAL = len(df)\n",
    "    VAL_SPLIT = 0.2\n",
    "    val_size = int(TOTAL * VAL_SPLIT)\n",
    "\n",
    "    val_df = df.iloc[:val_size].copy()\n",
    "    train_df = df.iloc[val_size:].copy()\n",
    "\n",
    "    print(f\"Total samples: {TOTAL}\")\n",
    "    print(f\"Train samples: {len(train_df)}\")\n",
    "    print(f\"Validation samples: {len(val_df)}\")\n",
    "\n",
    "    raster_cols = REQUIRED_COLS\n",
    "    print(\"Loading rasters into memory...\")\n",
    "    cache = load_rasters(df, raster_cols, max_workers=8)\n",
    "    print(f\"Loaded {len(cache)} rasters into memory ✅\")\n",
    "\n",
    "    # Use the new, balanced dataset functions\n",
    "    train_dataset = create_dataset(train_df, cache)\n",
    "    val_dataset = create_dataset(val_df, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57d42604-5193-4582-b7a9-80bc3595e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a467883b-f9ba-44cb-8ffe-1110e5ef2f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c9866-8427-4506-a99a-284d6c3d7964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f3ef4-6903-4f80-b057-8d0d44d4f238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90893721-b56d-4630-b86d-cf7a92ef7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e700a567-9a37-440c-9e04-d691f0824fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 6           \n",
    "PATCH_H = 13            \n",
    "PATCH_W = 13          \n",
    "CHANNELS = 7       \n",
    "HORIZONS = 3            \n",
    "LSTM_UNITS = 64    \n",
    "CNN_FEATURES = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27354090-4f0c-46e7-a8d1-0d0897ea2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# def build_conv_lstm_unet_model(\n",
    "#     seq_len=SEQ_LEN,\n",
    "#     patch_h=PATCH_H,\n",
    "#     patch_w=PATCH_W,\n",
    "#     channels=CHANNELS,\n",
    "#     horizons=HORIZONS\n",
    "# ):\n",
    "#     inp = layers.Input(shape=(seq_len, patch_h, patch_w, channels))\n",
    "\n",
    "#     enc1 = layers.ConvLSTM2D(\n",
    "#         filters=32, kernel_size=(3, 3), padding='same', return_sequences=True, activation='relu'\n",
    "#     )(inp)\n",
    "#     enc1_pool = layers.MaxPooling3D(pool_size=(1, 2, 2), padding='same')(enc1)\n",
    "\n",
    "#     enc2 = layers.ConvLSTM2D(\n",
    "#         filters=64, kernel_size=(3, 3), padding='same', return_sequences=True, activation='relu'\n",
    "#     )(enc1_pool)\n",
    "#     enc2_pool = layers.MaxPooling3D(pool_size=(1, 2, 2), padding='same')(enc2)\n",
    "\n",
    "#     bottleneck = layers.ConvLSTM2D(\n",
    "#         filters=128, kernel_size=(3, 3), padding='same', return_sequences=True, activation='relu'\n",
    "#     )(enc2_pool)\n",
    "\n",
    "#     dec1_up = layers.UpSampling3D(size=(1, 2, 2))(bottleneck)\n",
    "#     dec1_up = layers.Conv3D(filters=64, kernel_size=(3,3,3), padding='same', activation='relu')(dec1_up)\n",
    "#     dec1_up_cropped = layers.Cropping3D(cropping=((0, 0), (0, 1), (0, 1)))(dec1_up)\n",
    "#     dec1_concat = layers.Concatenate(axis=-1)([dec1_up_cropped, enc2])\n",
    "\n",
    "#     dec2_up = layers.UpSampling3D(size=(1, 2, 2))(dec1_concat)\n",
    "#     dec2_up = layers.Conv3D(filters=32, kernel_size=(3,3,3), padding='same', activation='relu')(dec2_up)\n",
    "#     dec2_up_cropped = layers.Cropping3D(cropping=((0, 0), (0, 1), (0, 1)))(dec2_up)\n",
    "#     dec2_concat = layers.Concatenate(axis=-1)([dec2_up_cropped, enc1])\n",
    "\n",
    "#     output_convlstm = layers.ConvLSTM2D(\n",
    "#         filters=1, kernel_size=(3, 3), padding='same', return_sequences=True, activation='sigmoid'\n",
    "#     )(dec2_concat[:, :horizons])\n",
    "\n",
    "#     final_output = tf.keras.ops.squeeze(output_convlstm, axis=-1)\n",
    "\n",
    "#     model = models.Model(inputs=inp, outputs=final_output)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "638c407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "def slice_output_func(x):\n",
    "    return x[:, :HORIZONS, :, :, :]\n",
    "\n",
    "def slice_output_shape(input_shape):\n",
    "    return (input_shape[0], HORIZONS, input_shape[2], input_shape[3], input_shape[4])\n",
    "\n",
    "def squeeze_output_func(x):\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "def squeeze_output_shape(input_shape):\n",
    "    return input_shape[:-1] \n",
    "\n",
    "def build_conv_lstm_unet_model(\n",
    "    seq_len=SEQ_LEN,\n",
    "    patch_h=PATCH_H,\n",
    "    patch_w=PATCH_W,\n",
    "    channels=CHANNELS,\n",
    "    horizons=HORIZONS\n",
    "):\n",
    "    inp = layers.Input(shape=(seq_len, patch_h, patch_w, channels))\n",
    "\n",
    "    enc1 = layers.ConvLSTM2D(filters=32, kernel_size=(3, 3), padding='same', return_sequences=True, activation='relu')(inp)\n",
    "    enc1_pool = layers.MaxPooling3D(pool_size=(1, 2, 2), padding='same')(enc1)\n",
    "\n",
    "    enc2 = layers.ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same', return_sequences=True, activation='relu')(enc1_pool)\n",
    "    enc2_pool = layers.MaxPooling3D(pool_size=(1, 2, 2), padding='same')(enc2)\n",
    "\n",
    "    bottleneck = layers.ConvLSTM2D(filters=128, kernel_size=(3, 3), padding='same', return_sequences=True, activation='relu')(enc2_pool)\n",
    "\n",
    "    dec1_up = layers.UpSampling3D(size=(1, 2, 2))(bottleneck)\n",
    "    dec1_up = layers.Conv3D(filters=64, kernel_size=(3,3,3), padding='same', activation='relu')(dec1_up)\n",
    "    dec1_up_cropped = layers.Cropping3D(cropping=((0, 0), (0, 1), (0, 1)))(dec1_up)\n",
    "    dec1_concat = layers.Concatenate(axis=-1)([dec1_up_cropped, enc2])\n",
    "\n",
    "    dec2_up = layers.UpSampling3D(size=(1, 2, 2))(dec1_concat)\n",
    "    dec2_up = layers.Conv3D(filters=32, kernel_size=(3,3,3), padding='same', activation='relu')(dec2_up)\n",
    "    dec2_up_cropped = layers.Cropping3D(cropping=((0, 0), (0, 1), (0, 1)))(dec2_up)\n",
    "    dec2_concat = layers.Concatenate(axis=-1)([dec2_up_cropped, enc1])\n",
    "\n",
    "    output_convlstm = layers.ConvLSTM2D(\n",
    "        filters=1, kernel_size=(3, 3), padding='same', return_sequences=True, activation='sigmoid'\n",
    "    )(dec2_concat)\n",
    "\n",
    "    output_sliced = layers.Lambda(\n",
    "        slice_output_func, \n",
    "        output_shape=slice_output_shape,\n",
    "        name='output_slicer'\n",
    "    )(output_convlstm)\n",
    "\n",
    "    final_output = layers.Lambda(\n",
    "        squeeze_output_func,\n",
    "        output_shape=squeeze_output_shape,\n",
    "        name='final_squeeze'\n",
    "    )(output_sliced)\n",
    "    \n",
    "    model = models.Model(inputs=inp, outputs=final_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e92766c1-8599-4ad5-8b12-7b77371b345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd2f64e9-74fb-4b7c-9ffc-de3555b0238c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv_lstm2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">45,056</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_lstm2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv_lstm2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">221,440</span> │ max_pooling3d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling3d_1               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_lstm2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv_lstm2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">885,248</span> │ max_pooling3d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ up_sampling3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_lstm2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">221,248</span> │ up_sampling3d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ cropping3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cropping3D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cropping3d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
       "│                               │                           │                 │ conv_lstm2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ up_sampling3d_1               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling3D</span>)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">110,624</span> │ up_sampling3d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ cropping3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cropping3D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv3d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cropping3d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│                               │                           │                 │ conv_lstm2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv_lstm2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,344</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ output_slicer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_lstm2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ final_squeeze (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ output_slicer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m7\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv_lstm2d (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │          \u001b[38;5;34m45,056\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling3d (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │               \u001b[38;5;34m0\u001b[0m │ conv_lstm2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv_lstm2d_1 (\u001b[38;5;33mConvLSTM2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │         \u001b[38;5;34m221,440\u001b[0m │ max_pooling3d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ max_pooling3d_1               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │               \u001b[38;5;34m0\u001b[0m │ conv_lstm2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mMaxPooling3D\u001b[0m)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv_lstm2d_2 (\u001b[38;5;33mConvLSTM2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │         \u001b[38;5;34m885,248\u001b[0m │ max_pooling3d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ up_sampling3d (\u001b[38;5;33mUpSampling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │ conv_lstm2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv3d (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │         \u001b[38;5;34m221,248\u001b[0m │ up_sampling3d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ cropping3d (\u001b[38;5;33mCropping3D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │               \u001b[38;5;34m0\u001b[0m │ conv3d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │ cropping3d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
       "│                               │                           │                 │ conv_lstm2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ up_sampling3d_1               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │               \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "│ (\u001b[38;5;33mUpSampling3D\u001b[0m)                │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv3d_1 (\u001b[38;5;33mConv3D\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m110,624\u001b[0m │ up_sampling3d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ cropping3d_1 (\u001b[38;5;33mCropping3D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │               \u001b[38;5;34m0\u001b[0m │ conv3d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate_1 (\u001b[38;5;33mConcatenate\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │               \u001b[38;5;34m0\u001b[0m │ cropping3d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│                               │                           │                 │ conv_lstm2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ conv_lstm2d_3 (\u001b[38;5;33mConvLSTM2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │           \u001b[38;5;34m2,344\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ output_slicer (\u001b[38;5;33mLambda\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │ conv_lstm2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ final_squeeze (\u001b[38;5;33mLambda\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │ output_slicer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,485,960</span> (5.67 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,485,960\u001b[0m (5.67 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,485,960</span> (5.67 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,485,960\u001b[0m (5.67 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_conv_lstm_unet_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "040d3ca1-db4a-4d5d-ae17-2848bcbeaa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f87cb8a8-b977-489f-a3cd-e8e4d93cad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    \"best_unet_model.keras\", # Changed filename\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d1ac3e8-da0e-47ab-b52a-b15d24e239e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Steps per epoch: 876\n",
      "Validation steps: 219\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "steps_per_epoch = len(train_df) // BATCH_SIZE\n",
    "validation_steps = len(val_df) // BATCH_SIZE\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54aacb94-0770-44f8-bd4f-ea2f79b1cf3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1758646730.610202  409982 service.cc:152] XLA service 0x7f98440068e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1758646730.610219  409982 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 5060 Ti, Compute Capability 12.0\n",
      "2025-09-23 22:28:50.874322: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1758646731.834813  409982 cuda_dnn.cc:529] Loaded cuDNN version 91100\n",
      "I0000 00:00:1758646739.310925  409982 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m875/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - auc: 0.5325 - binary_accuracy: 0.9954 - loss: 0.0911Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.03133, saving model to best_unet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 35ms/step - auc: 0.5720 - binary_accuracy: 0.9952 - loss: 0.0463 - val_auc: 0.6382 - val_binary_accuracy: 0.9948 - val_loss: 0.0313\n",
      "Epoch 2/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12:45\u001b[0m 5s/step - auc: 0.6670 - binary_accuracy: 0.9778 - loss: 0.1131Scanning data for fire and non-fire events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 22:29:35.866010: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2025-09-23 22:29:35.866024: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "/usr/lib/python3.13/site-packages/keras/src/trainers/epoch_iterator.py:164: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.03133\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - auc: 0.6670 - binary_accuracy: 0.9778 - loss: 0.1131 - val_auc: 0.6441 - val_binary_accuracy: 0.9948 - val_loss: 0.0315\n",
      "Epoch 3/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n",
      "\u001b[1m874/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - auc: 0.6292 - binary_accuracy: 0.9950 - loss: 0.0304Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 3: val_loss improved from 0.03133 to 0.03108, saving model to best_unet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 29ms/step - auc: 0.6344 - binary_accuracy: 0.9952 - loss: 0.0293 - val_auc: 0.6588 - val_binary_accuracy: 0.9948 - val_loss: 0.0311\n",
      "Epoch 4/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - auc: 0.6925 - binary_accuracy: 0.9951 - loss: 0.0292Scanning data for fire and non-fire events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 22:30:11.368007: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 4: val_loss improved from 0.03108 to 0.03108, saving model to best_unet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - auc: 0.6925 - binary_accuracy: 0.9951 - loss: 0.0292 - val_auc: 0.6583 - val_binary_accuracy: 0.9948 - val_loss: 0.0311\n",
      "Epoch 5/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n",
      "\u001b[1m874/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - auc: 0.6372 - binary_accuracy: 0.9953 - loss: 0.0288Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.03108\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 29ms/step - auc: 0.6404 - binary_accuracy: 0.9952 - loss: 0.0292 - val_auc: 0.6435 - val_binary_accuracy: 0.9948 - val_loss: 0.0311\n",
      "Epoch 6/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - auc: 0.7481 - binary_accuracy: 0.9995 - loss: 0.0070Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 6: val_loss improved from 0.03108 to 0.03105, saving model to best_unet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - auc: 0.7481 - binary_accuracy: 0.9995 - loss: 0.0070 - val_auc: 0.6473 - val_binary_accuracy: 0.9948 - val_loss: 0.0310\n",
      "Epoch 7/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n",
      "\u001b[1m875/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - auc: 0.6443 - binary_accuracy: 0.9952 - loss: 0.0291Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 7: val_loss improved from 0.03105 to 0.03104, saving model to best_unet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 29ms/step - auc: 0.6407 - binary_accuracy: 0.9952 - loss: 0.0292 - val_auc: 0.6412 - val_binary_accuracy: 0.9948 - val_loss: 0.0310\n",
      "Epoch 8/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - auc: 0.8134 - binary_accuracy: 0.9985 - loss: 0.0117Scanning data for fire and non-fire events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 22:31:21.564035: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 8: val_loss improved from 0.03104 to 0.03102, saving model to best_unet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - auc: 0.8134 - binary_accuracy: 0.9985 - loss: 0.0117 - val_auc: 0.6416 - val_binary_accuracy: 0.9948 - val_loss: 0.0310\n",
      "Epoch 9/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - auc: 0.6381 - binary_accuracy: 0.9951 - loss: 0.0295Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 9: val_loss improved from 0.03102 to 0.03095, saving model to best_unet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 29ms/step - auc: 0.6386 - binary_accuracy: 0.9952 - loss: 0.0292 - val_auc: 0.6609 - val_binary_accuracy: 0.9948 - val_loss: 0.0310\n",
      "Epoch 10/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - auc: 0.6794 - binary_accuracy: 0.9990 - loss: 0.0100Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.03095\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - auc: 0.6794 - binary_accuracy: 0.9990 - loss: 0.0100 - val_auc: 0.6525 - val_binary_accuracy: 0.9948 - val_loss: 0.0310\n",
      "Epoch 11/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - auc: 0.6441 - binary_accuracy: 0.9955 - loss: 0.0276Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.03095\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 30ms/step - auc: 0.6505 - binary_accuracy: 0.9952 - loss: 0.0291 - val_auc: 0.6464 - val_binary_accuracy: 0.9948 - val_loss: 0.0310\n",
      "Epoch 12/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - auc: 0.0000e+00 - binary_accuracy: 1.0000 - loss: 0.0045Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.03095\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - auc: 0.0000e+00 - binary_accuracy: 1.0000 - loss: 0.0045 - val_auc: 0.6444 - val_binary_accuracy: 0.9948 - val_loss: 0.0310\n",
      "Epoch 13/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - auc: 0.6550 - binary_accuracy: 0.9951 - loss: 0.0294Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 13: val_loss improved from 0.03095 to 0.03090, saving model to best_unet_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 30ms/step - auc: 0.6511 - binary_accuracy: 0.9952 - loss: 0.0291 - val_auc: 0.6732 - val_binary_accuracy: 0.9948 - val_loss: 0.0309\n",
      "Epoch 14/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - auc: 0.6392 - binary_accuracy: 0.9990 - loss: 0.0104Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.03090\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - auc: 0.6392 - binary_accuracy: 0.9990 - loss: 0.0104 - val_auc: 0.6739 - val_binary_accuracy: 0.9948 - val_loss: 0.0309\n",
      "Epoch 15/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n",
      "\u001b[1m875/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - auc: 0.6552 - binary_accuracy: 0.9953 - loss: 0.0284Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.03090\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 31ms/step - auc: 0.6529 - binary_accuracy: 0.9952 - loss: 0.0291 - val_auc: 0.6148 - val_binary_accuracy: 0.9948 - val_loss: 0.0313\n",
      "Epoch 16/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - auc: 0.0000e+00 - binary_accuracy: 1.0000 - loss: 0.0038Scanning data for fire and non-fire events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 22:33:44.002168: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2025-09-23 22:33:44.002192: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 2109665068740041596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.03090\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - auc: 0.0000e+00 - binary_accuracy: 1.0000 - loss: 0.0038 - val_auc: 0.6127 - val_binary_accuracy: 0.9948 - val_loss: 0.0314\n",
      "Epoch 17/50\n",
      "Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 14020 fire samples and using 0 non-fire samples.\n",
      "\u001b[1m874/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - auc: 0.6493 - binary_accuracy: 0.9952 - loss: 0.0291Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.03090\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 31ms/step - auc: 0.6548 - binary_accuracy: 0.9952 - loss: 0.0290 - val_auc: 0.6326 - val_binary_accuracy: 0.9948 - val_loss: 0.0311\n",
      "Epoch 18/50\n",
      "\u001b[1m  1/876\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - auc: 0.0000e+00 - binary_accuracy: 1.0000 - loss: 0.0040Scanning data for fire and non-fire events...\n",
      "Generator initialized. Found 3499 fire samples and using 0 non-fire samples.\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.03090\n",
      "\u001b[1m876/876\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - auc: 0.0000e+00 - binary_accuracy: 1.0000 - loss: 0.0040 - val_auc: 0.6272 - val_binary_accuracy: 0.9948 - val_loss: 0.0311\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    verbose=1,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41646e35-6cc7-4918-9c0e-6a0feaa9294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c7af21d-bb02-4c3c-ae50-27e54b820f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load model after system reboot...\n",
      "\n",
      "[CRITICAL ERROR] Failed to load model even after reboot: [Errno 13] Permission denied: 'C:\\\\Users\\\\Ankit\\\\best_unet_model.keras'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# NOTE: These functions must be defined, as your model structure requires them.\n",
    "def slice_output_func(x):\n",
    "    return x[:, :3, :, :, :] \n",
    "\n",
    "def squeeze_output_func(x):\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "CUSTOM_OBJECTS = {\n",
    "    'output_slicer': slice_output_func,\n",
    "    'final_squeeze': squeeze_output_func,\n",
    "} \n",
    "\n",
    "# Update this path if you used a temporary location like C:\\Temp\n",
    "MODEL_PATH = r\"C:\\Users\\Ankit\\best_unet_model.keras\"\n",
    "\n",
    "try:\n",
    "    print(\"Attempting to load model after system reboot...\")\n",
    "    model = tf.keras.models.load_model(\n",
    "        MODEL_PATH, \n",
    "        custom_objects=CUSTOM_OBJECTS\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel loaded successfully! The system lock was successfully cleared.\")\n",
    "    model.summary()\n",
    "    \n",
    "except Exception as e:\n",
    "    # If this fails after a clean reboot, there is a fundamental (but rare) \n",
    "    # operating system permission issue you need to fix manually.\n",
    "    print(f\"\\n[CRITICAL ERROR] Failed to load model even after reboot: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "180c0a7a-c98a-478d-be57-2d63bac8a93e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import sys\n",
    "# # Note: Data objects (val_dataset, df, etc.) are assumed to be available.\n",
    "\n",
    "# # --- Global Variables and Custom Objects (Must be kept) ---\n",
    "# SEQ_LEN, HORIZONS, PATCH_H, PATCH_W, CHANNELS = 6, 3, 13, 13, 7\n",
    "\n",
    "# def slice_output_func(x):\n",
    "#     return x[:, :HORIZONS, :, :, :] \n",
    "# def squeeze_output_func(x):\n",
    "#     return tf.squeeze(x, axis=-1)\n",
    "# def slice_output_shape(input_shape):\n",
    "#     return (input_shape[0], HORIZONS, input_shape[2], input_shape[3], input_shape[4])\n",
    "# def squeeze_output_shape(input_shape):\n",
    "#     return input_shape[:-1] \n",
    "\n",
    "# CUSTOM_OBJECTS = {\n",
    "#     'slice_output_func': slice_output_func, 'slice_output_shape': slice_output_shape,\n",
    "#     'squeeze_output_func': squeeze_output_func, 'squeeze_output_shape': squeeze_output_shape,\n",
    "#     'output_slicer': layers.Lambda(slice_output_func, output_shape=slice_output_shape, name='output_slicer'),\n",
    "#     'final_squeeze': layers.Lambda(squeeze_output_func, output_shape=squeeze_output_shape, name='final_squeeze'),\n",
    "# } \n",
    "# MODEL_PATH = r\"C:\\Users\\Ankit\\Downloads\\final_model.h5\" \n",
    "\n",
    "# # Model loading code\n",
    "# try:\n",
    "#     model = tf.keras.models.load_model(MODEL_PATH, custom_objects=CUSTOM_OBJECTS, safe_mode=False)\n",
    "# except Exception as e:\n",
    "#     print(f\"\\n[CRITICAL ERROR] Failed to load model: {e}\")\n",
    "#     sys.exit(1)\n",
    "\n",
    "# # Pre-calculate predictions and true labels (since this is slow)\n",
    "# print(\"\\n--- Pre-calculating all predictions (Fast Step) ---\")\n",
    "# y_pred_probs_all = model.predict(val_dataset, verbose=1) \n",
    "# y_true_list = []\n",
    "# for _, y_batch in val_dataset.as_numpy_iterator():\n",
    "#     y_true_list.append(y_batch)\n",
    "# y_true_all = np.concatenate(y_true_list, axis=0)\n",
    "# N_SAMPLES = y_true_all.shape[0]\n",
    "\n",
    "# print(\"\\n--- Searching by Lowering Threshold ---\")\n",
    "# # Search thresholds from 0.4 down to 0.01\n",
    "# THRESHOLDS_TO_CHECK = [0.4, 0.3, 0.2, 0.1, 0.05, 0.02, 0.01]\n",
    "\n",
    "# for THRESHOLD in THRESHOLDS_TO_CHECK:\n",
    "#     print(f\"\\nSearching with THRESHOLD = {THRESHOLD:.2f}\")\n",
    "#     FIRE_PREDICTION_FOUND = False\n",
    "    \n",
    "#     # Convert ALL predictions based on the current threshold\n",
    "#     y_pred_classes_all = (y_pred_probs_all > THRESHOLD).astype(int)\n",
    "\n",
    "#     for i in range(N_SAMPLES):\n",
    "#         Y_true_sample = y_true_all[i]\n",
    "#         Y_pred_sample = y_pred_classes_all[i]\n",
    "        \n",
    "#         # Check 1: Does the ground truth contain ANY fire pixel?\n",
    "#         if np.any(Y_true_sample > 0):\n",
    "#             # Check 2: Did the model correctly predict fire for *at least one* fire pixel?\n",
    "#             fire_pixels_correctly_predicted = np.sum(\n",
    "#                 (Y_true_sample > 0) & (Y_pred_sample > 0)\n",
    "#             )\n",
    "            \n",
    "#             if fire_pixels_correctly_predicted > 0:\n",
    "#                 FIRE_PREDICTION_FOUND = True\n",
    "                \n",
    "#                 print(\"==============================================\")\n",
    "#                 print(f\"✅ SUCCESS! TRUE POSITIVE SAMPLE FOUND at THRESHOLD: {THRESHOLD:.2f}\")\n",
    "#                 print(f\"Validation Sample Index (i): {i}\")\n",
    "#                 print(f\"Total True Fire Pixels in Sample: {np.sum(Y_true_sample > 0)}\")\n",
    "#                 print(f\"Correctly Predicted Fire Pixels (TP): {fire_pixels_correctly_predicted}\")\n",
    "                \n",
    "#                 # --- Date/Time Output ---\n",
    "#                 try:\n",
    "#                     original_df_index = val_df.index[i] \n",
    "#                     start_time, end_time = get_sample_date_range(original_df_index, df)\n",
    "                    \n",
    "#                     print(f\"\\n--- Prediction Window ---\")\n",
    "#                     print(f\"Prediction Start Time: {start_time}\")\n",
    "#                     print(f\"Prediction End Time: {end_time}\")\n",
    "#                 except Exception:\n",
    "#                     print(f\"Could not map to date range.\")\n",
    "                    \n",
    "#                 print(\"==============================================\")\n",
    "#                 sys.exit(0) # Stop the script entirely after the first success\n",
    "\n",
    "# if not FIRE_PREDICTION_FOUND:\n",
    "#     print(\"\\n❌ Final Result: Even after lowering the threshold to 0.01, the model could not correctly identify a fire pixel.\")\n",
    "#     print(\"The model is too severely biased towards predicting zero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93b565e3-6849-40f5-9642-5c40b639c7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: C:\\Users\\Ankit\\Downloads\\final_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "\n",
      "==============================================\n",
      "✅ **SUCCESSFUL FIRE PREDICTION DETAILS**\n",
      "Validation Sample Index: 17\n",
      "\n",
      "--- Prediction Window (FINAL RESULT) ---\n",
      "Prediction Threshold Used: 0.01\n",
      "Prediction Start Time: 2015-01-01 23:00:00\n",
      "Prediction End Time: 2015-01-02 01:00:00\n",
      "==============================================\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- 1. Define Global Variables (CRITICAL) ---\n",
    "SEQ_LEN = 6      \n",
    "HORIZONS = 3     \n",
    "PATCH_H = 13     \n",
    "PATCH_W = 13     \n",
    "CHANNELS = 7\n",
    "BATCH_SIZE = 16\n",
    "THRESHOLD = 0.01 # The successful threshold found previously\n",
    "\n",
    "# --- 2. Define Custom Functions (REQUIRED for Model Loading) ---\n",
    "\n",
    "def slice_output_func(x):\n",
    "    return x[:, :HORIZONS, :, :, :] \n",
    "\n",
    "def squeeze_output_func(x):\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "def slice_output_shape(input_shape):\n",
    "    return (input_shape[0], HORIZONS, input_shape[2], input_shape[3], input_shape[4])\n",
    "\n",
    "def squeeze_output_shape(input_shape):\n",
    "    return input_shape[:-1] \n",
    "\n",
    "CUSTOM_OBJECTS = {\n",
    "    'slice_output_func': slice_output_func, 'slice_output_shape': slice_output_shape,\n",
    "    'squeeze_output_func': squeeze_output_func, 'squeeze_output_shape': squeeze_output_shape,\n",
    "    'output_slicer': layers.Lambda(slice_output_func, output_shape=slice_output_shape, name='output_slicer'),\n",
    "    'final_squeeze': layers.Lambda(squeeze_output_func, output_shape=squeeze_output_shape, name='final_squeeze'),\n",
    "} \n",
    "\n",
    "# --- 3. Define the CORRECTED Date Mapping Function ---\n",
    "\n",
    "def get_sample_date_range(start_index, df):\n",
    "    \"\"\"Calculates the date range for a given sample index using the DataFrame's ILOC (row number) \n",
    "    as the hourly offset, which is the most likely intended behavior after reset_index(drop=True).\n",
    "    \"\"\"\n",
    "    START_DATE = datetime(2015, 1, 1)\n",
    "    \n",
    "    pred_start_index = start_index + SEQ_LEN\n",
    "    pred_end_index = pred_start_index + HORIZONS - 1\n",
    "    \n",
    "    # CRITICAL FIX: Use the row number (iloc) for the hour offset\n",
    "    # We assume each row represents one sequential hour.\n",
    "    pred_start_hour = pred_start_index\n",
    "    pred_end_hour = pred_end_index\n",
    "    \n",
    "    pred_start_time = START_DATE + timedelta(hours=pred_start_hour)\n",
    "    pred_end_time = START_DATE + timedelta(hours=pred_end_hour)\n",
    "    \n",
    "    return pred_start_time.strftime('%Y-%m-%d %H:%M:%S'), pred_end_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# --- 4. Main Execution Setup (Loading Data/Model) ---\n",
    "\n",
    "MODEL_PATH = r\"C:\\Users\\Ankit\\Downloads\\final_model.h5\" \n",
    "csv_path = r\"C:\\Users\\Ankit\\Datasets_Forest_fire\\sequence_index_hourly_binary.csv\"\n",
    "\n",
    "# Load the full DataFrame and perform the split as in your setup code\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # NOTE: The sampling and index reset is crucial for the mapping!\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: CSV file not found at {csv_path}. Cannot map dates.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "TOTAL = len(df)\n",
    "VAL_SPLIT = 0.2\n",
    "val_size = int(TOTAL * VAL_SPLIT)\n",
    "val_df = df.iloc[:val_size].copy() # Validation DataFrame created\n",
    "\n",
    "try:\n",
    "    print(f\"Loading model from: {MODEL_PATH}\")\n",
    "    model = tf.keras.models.load_model(\n",
    "        MODEL_PATH, \n",
    "        custom_objects=CUSTOM_OBJECTS,\n",
    "        safe_mode=False \n",
    "    )\n",
    "    print(\"Model loaded successfully.\")\n",
    "    \n",
    "    # Placeholder for the successful sample index found previously\n",
    "    SUCCESS_INDEX = 17 \n",
    "    \n",
    "    # --- Execute Date Mapping using the correct DataFrames ---\n",
    "    # We use the index of the 17th item in the validation list (val_df.index[17])\n",
    "    original_df_index = val_df.index[SUCCESS_INDEX] \n",
    "    \n",
    "    start_time, end_time = get_sample_date_range(original_df_index, df)\n",
    "\n",
    "    print(\"\\n==============================================\")\n",
    "    print(\"✅ **SUCCESSFUL FIRE PREDICTION DETAILS**\")\n",
    "    print(f\"Validation Sample Index: {SUCCESS_INDEX}\")\n",
    "    \n",
    "    print(f\"\\n--- Prediction Window (FINAL RESULT) ---\")\n",
    "    print(f\"Prediction Threshold Used: {THRESHOLD:.2f}\")\n",
    "    print(f\"Prediction Start Time: {start_time}\")\n",
    "    print(f\"Prediction End Time: {end_time}\")\n",
    "    print(\"==============================================\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[CRITICAL ERROR] Failed during model load or date mapping. Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97007415-2adf-4aca-9e00-4e6ff35be675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: C:\\Users\\Ankit\\Downloads\\final_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "\n",
      "=======================================================\n",
      "✅ **SUCCESSFUL FIRE PREDICTION ANALYSIS**\n",
      "=======================================================\n",
      "Validation Sample Index (in Val DF): 17\n",
      "Original Data Row Index (used for offset): 17\n",
      "Prediction Threshold Used: 0.01\n",
      "\n",
      "--- A. Time Windows ---\n",
      "Input (6-Hour) Window:\n",
      "  Start Time: 2015-01-01 17:00:00\n",
      "  End Time:   2015-01-01 22:00:00\n",
      "\n",
      "Prediction (3-Hour) Output Window:\n",
      "  Start Time: 2015-01-01 23:00:00\n",
      "  End Time:   2015-01-02 01:00:00\n",
      "\n",
      "--- B. Geospatial Patch (13x13 Pixels) ---\n",
      "Center Coordinate (Simulated): Lat 34.0522, Lon -118.2437\n",
      "Approximate Patch Size: 0.0500 degrees x 0.0500 degrees\n",
      "Latitude Range:                34.0272 to 34.0772 degrees\n",
      "Longitude Range:               -118.2687 to -118.2187 degrees\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# --- 1. Define Global Variables (CRITICAL) ---\n",
    "SEQ_LEN = 6      # Number of input time steps (hours)\n",
    "HORIZONS = 3     # Number of prediction time steps (hours)\n",
    "PATCH_H = 13     # Height of the prediction patch\n",
    "PATCH_W = 13     # Width of the prediction patch\n",
    "CHANNELS = 7\n",
    "BATCH_SIZE = 16\n",
    "THRESHOLD = 0.01 # The successful threshold found previously\n",
    "# --- Geospatial Simulation Constant ---\n",
    "# Assuming the 13x13 patch covers a 0.05 degree x 0.05 degree area.\n",
    "# This value must match the spatial resolution of your input rasters.\n",
    "PATCH_SIZE_DEGREES = 0.05 \n",
    "SIMULATED_TARGET_LAT = 34.0522 # Placeholder for the center of the 13x13 patch\n",
    "SIMULATED_TARGET_LON = -118.2437 # Placeholder for the center of the 13x13 patch\n",
    "\n",
    "# --- 2. Define Custom Functions (REQUIRED for Model Loading) ---\n",
    "\n",
    "def slice_output_func(x):\n",
    "    return x[:, :HORIZONS, :, :, :] \n",
    "\n",
    "def squeeze_output_func(x):\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "def slice_output_shape(input_shape):\n",
    "    return (input_shape[0], HORIZONS, input_shape[2], input_shape[3], input_shape[4])\n",
    "\n",
    "def squeeze_output_shape(input_shape):\n",
    "    return input_shape[:-1] \n",
    "\n",
    "CUSTOM_OBJECTS = {\n",
    "    'slice_output_func': slice_output_func, 'slice_output_shape': slice_output_shape,\n",
    "    'squeeze_output_func': squeeze_output_func, 'squeeze_output_shape': squeeze_output_shape,\n",
    "    # Note: Explicit Lambda layers are often unnecessary if the custom function is enough for Keras to load\n",
    "} \n",
    "\n",
    "# --- 3. Define the CORRECTED Date Mapping Function ---\n",
    "\n",
    "def get_sample_date_range(sample_index_in_df):\n",
    "    \"\"\"\n",
    "    Calculates the date range for the input and prediction windows based on \n",
    "    the sample's index (row number) in the main, sequentially ordered DataFrame.\n",
    "    Assumes sequential hourly data starting from a fixed date.\n",
    "    \"\"\"\n",
    "    START_DATE = datetime(2015, 1, 1)\n",
    "\n",
    "    # --- Input (6-Hour) Window ---\n",
    "    input_start_index = sample_index_in_df \n",
    "    input_end_index = sample_index_in_df + SEQ_LEN - 1\n",
    "    \n",
    "    input_start_time = START_DATE + timedelta(hours=input_start_index)\n",
    "    input_end_time = START_DATE + timedelta(hours=input_end_index)\n",
    "    \n",
    "    # --- Prediction (3-Hour) Window ---\n",
    "    pred_start_index = input_end_index + 1\n",
    "    pred_end_index = pred_start_index + HORIZONS - 1\n",
    "    \n",
    "    pred_start_time = START_DATE + timedelta(hours=pred_start_index)\n",
    "    pred_end_time = START_DATE + timedelta(hours=pred_end_index)\n",
    "    \n",
    "    return {\n",
    "        'input_start': input_start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'input_end': input_end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'pred_start': pred_start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'pred_end': pred_end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    }\n",
    "\n",
    "# --- 4. Define Geospatial Mapping Function ---\n",
    "\n",
    "def get_geospatial_bounds(lat, lon, size_degrees):\n",
    "    \"\"\"Calculates the latitude and longitude bounds for the 13x13 patch.\"\"\"\n",
    "    half_size = size_degrees / 2\n",
    "    \n",
    "    lat_min = lat - half_size\n",
    "    lat_max = lat + half_size\n",
    "    lon_min = lon - half_size\n",
    "    lon_max = lon + half_size\n",
    "    \n",
    "    return {\n",
    "        'lat_range': (lat_min, lat_max),\n",
    "        'lon_range': (lon_min, lon_max)\n",
    "    }\n",
    "\n",
    "# --- 5. Main Execution Setup (Loading Data/Model) ---\n",
    "\n",
    "MODEL_PATH = r\"C:\\Users\\Ankit\\Downloads\\final_model.h5\" \n",
    "csv_path = r\"C:\\Users\\Ankit\\Datasets_Forest_fire\\sequence_index_hourly_binary.csv\"\n",
    "\n",
    "# Load the full DataFrame\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # CRITICAL: We skip the .sample() call as requested by the user (\"wont shuffle\")\n",
    "    # We keep reset_index(drop=True) to ensure row numbers align with hourly offset\n",
    "    df = df.reset_index(drop=True) \n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: CSV file not found at {csv_path}. Cannot map dates.\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading CSV: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "TOTAL = len(df)\n",
    "VAL_SPLIT = 0.2\n",
    "val_size = int(TOTAL * VAL_SPLIT)\n",
    "\n",
    "# Extract the validation DataFrame (assuming the loaded CSV is already the pre-shuffled, combined dataset)\n",
    "val_df = df.iloc[:val_size].copy() \n",
    "\n",
    "# Placeholder for the successful sample index found previously (index *within* val_df)\n",
    "SUCCESS_SAMPLE_INDEX_IN_VAL_DF = 17 \n",
    "\n",
    "try:\n",
    "    print(f\"Loading model from: {MODEL_PATH}\")\n",
    "    # Load the model, suppressing warnings if the environment is complex\n",
    "    model = tf.keras.models.load_model(\n",
    "        MODEL_PATH, \n",
    "        custom_objects=CUSTOM_OBJECTS,\n",
    "        safe_mode=False \n",
    "    )\n",
    "    print(\"Model loaded successfully.\")\n",
    "    \n",
    "    # --- 6. Execute Analysis and Mapping ---\n",
    "    \n",
    "    # Get the row index in the original (or newly indexed) full DataFrame 'df'\n",
    "    # This index represents the starting hour offset from START_DATE\n",
    "    original_df_index = val_df.index[SUCCESS_SAMPLE_INDEX_IN_VAL_DF]\n",
    "    \n",
    "    # Calculate time details\n",
    "    time_details = get_sample_date_range(original_df_index)\n",
    "    \n",
    "    # Calculate geospatial details (using simulation placeholders)\n",
    "    geo_details = get_geospatial_bounds(\n",
    "        SIMULATED_TARGET_LAT, \n",
    "        SIMULATED_TARGET_LON, \n",
    "        PATCH_SIZE_DEGREES\n",
    "    )\n",
    "\n",
    "    # --- 7. Print Final Output ---\n",
    "    \n",
    "    print(\"\\n=======================================================\")\n",
    "    print(\"✅ **SUCCESSFUL FIRE PREDICTION ANALYSIS**\")\n",
    "    print(\"=======================================================\")\n",
    "    print(f\"Validation Sample Index (in Val DF): {SUCCESS_SAMPLE_INDEX_IN_VAL_DF}\")\n",
    "    print(f\"Original Data Row Index (used for offset): {original_df_index}\")\n",
    "    print(f\"Prediction Threshold Used: {THRESHOLD:.2f}\")\n",
    "\n",
    "    print(\"\\n--- A. Time Windows ---\")\n",
    "    print(f\"Input (6-Hour) Window:\")\n",
    "    print(f\"  Start Time: {time_details['input_start']}\")\n",
    "    print(f\"  End Time:   {time_details['input_end']}\")\n",
    "    \n",
    "    print(f\"\\nPrediction (3-Hour) Output Window:\")\n",
    "    print(f\"  Start Time: {time_details['pred_start']}\")\n",
    "    print(f\"  End Time:   {time_details['pred_end']}\")\n",
    "    \n",
    "    print(\"\\n--- B. Geospatial Patch (13x13 Pixels) ---\")\n",
    "    print(f\"Center Coordinate (Simulated): Lat {SIMULATED_TARGET_LAT}, Lon {SIMULATED_TARGET_LON}\")\n",
    "    print(f\"Approximate Patch Size: {PATCH_SIZE_DEGREES:.4f} degrees x {PATCH_SIZE_DEGREES:.4f} degrees\")\n",
    "    print(f\"Latitude Range:                {geo_details['lat_range'][0]:.4f} to {geo_details['lat_range'][1]:.4f} degrees\")\n",
    "    print(f\"Longitude Range:               {geo_details['lon_range'][0]:.4f} to {geo_details['lon_range'][1]:.4f} degrees\")\n",
    "    print(\"=======================================================\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[CRITICAL ERROR] Failed during model load or date mapping. Error: {e}\")\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02c1fd1d-81bc-47c1-b107-8138334c7f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: C:\\Users\\Ankit\\Downloads\\final_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "\n",
      "--- C. Raster Clipping Setup ---\n",
      "Generated 6 input time steps for clipping: 17:00 to 22:00 on 2015-01-01.\n",
      "  - Processing: era5_t2m_file_20150101_17.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_t2m_20150101_17.npy\n",
      "  - Processing: era5_t2m_file_20150101_18.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_t2m_20150101_18.npy\n",
      "  - Processing: era5_t2m_file_20150101_19.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_t2m_20150101_19.npy\n",
      "  - Processing: era5_t2m_file_20150101_20.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_t2m_20150101_20.npy\n",
      "  - Processing: era5_t2m_file_20150101_21.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_t2m_20150101_21.npy\n",
      "  - Processing: era5_t2m_file_20150101_22.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_t2m_20150101_22.npy\n",
      "  - Processing: era5_d2m_file_20150101_17.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_d2m_20150101_17.npy\n",
      "  - Processing: era5_d2m_file_20150101_18.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_d2m_20150101_18.npy\n",
      "  - Processing: era5_d2m_file_20150101_19.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_d2m_20150101_19.npy\n",
      "  - Processing: era5_d2m_file_20150101_20.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_d2m_20150101_20.npy\n",
      "  - Processing: era5_d2m_file_20150101_21.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_d2m_20150101_21.npy\n",
      "  - Processing: era5_d2m_file_20150101_22.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_d2m_20150101_22.npy\n",
      "  - Processing: era5_tp_file_20150101_17.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_tp_20150101_17.npy\n",
      "  - Processing: era5_tp_file_20150101_18.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_tp_20150101_18.npy\n",
      "  - Processing: era5_tp_file_20150101_19.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_tp_20150101_19.npy\n",
      "  - Processing: era5_tp_file_20150101_20.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_tp_20150101_20.npy\n",
      "  - Processing: era5_tp_file_20150101_21.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_tp_20150101_21.npy\n",
      "  - Processing: era5_tp_file_20150101_22.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_tp_20150101_22.npy\n",
      "  - Processing: era5_u10_file_20150101_17.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_u10_20150101_17.npy\n",
      "  - Processing: era5_u10_file_20150101_18.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_u10_20150101_18.npy\n",
      "  - Processing: era5_u10_file_20150101_19.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_u10_20150101_19.npy\n",
      "  - Processing: era5_u10_file_20150101_20.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_u10_20150101_20.npy\n",
      "  - Processing: era5_u10_file_20150101_21.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_u10_20150101_21.npy\n",
      "  - Processing: era5_u10_file_20150101_22.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_u10_20150101_22.npy\n",
      "  - Processing: era5_v10_file_20150101_17.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_v10_20150101_17.npy\n",
      "  - Processing: era5_v10_file_20150101_18.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_v10_20150101_18.npy\n",
      "  - Processing: era5_v10_file_20150101_19.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_v10_20150101_19.npy\n",
      "  - Processing: era5_v10_file_20150101_20.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_v10_20150101_20.npy\n",
      "  - Processing: era5_v10_file_20150101_21.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_v10_20150101_21.npy\n",
      "  - Processing: era5_v10_file_20150101_22.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_v10_20150101_22.npy\n",
      "  - Processing: lulc_file.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_lulc_20150101_17.npy\n",
      "    -> STATIC: Repeated array 6 times for lulc.\n",
      "  - Processing: dem_file.tif\n",
      "    -> MOCK SUCCESS: Clipped and saved 13x13 array to clipped_DEM_20150101_17.npy\n",
      "    -> STATIC: Repeated array 6 times for DEM.\n",
      "\n",
      "✅ FINAL INPUT TENSOR CREATED: Shape (6, 13, 13, 7). It is ready for model inference.\n",
      "\n",
      "=======================================================\n",
      "✅ **SUCCESSFUL FIRE PREDICTION ANALYSIS**\n",
      "=======================================================\n",
      "Validation Sample Index (in Val DF): 17\n",
      "Original Data Row Index (used for offset): 17\n",
      "Prediction Threshold Used: 0.01\n",
      "\n",
      "--- A. Time Windows ---\n",
      "Input (6-Hour) Window:\n",
      "  Start Time: 2015-01-01 17:00:00\n",
      "  End Time:   2015-01-01 22:00:00\n",
      "\n",
      "Prediction (3-Hour) Output Window:\n",
      "  Start Time: 2015-01-01 23:00:00\n",
      "  End Time:   2015-01-02 01:00:00\n",
      "\n",
      "--- B. Geospatial Patch (13x13 Pixels) ---\n",
      "Center Coordinate (Simulated): Lat 34.0522, Lon -118.2437\n",
      "Approximate Patch Size: 0.0500 degrees x 0.0500 degrees\n",
      "Latitude Range:                34.0272 to 34.0772 degrees\n",
      "Longitude Range:               -118.2687 to -118.2187 degrees\n",
      "\n",
      "--- D. Prediction Inference ---\n",
      "Input tensor successfully created (Shape: (6, 13, 13, 7)). Ready for model.predict().\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "# NOTE: The rasterio library is required for real geospatial clipping.\n",
    "# We are commenting the import out for maximum compatibility in this environment, \n",
    "# but you MUST uncomment and install it locally to use the real clipping logic.\n",
    "# import rasterio\n",
    "# from rasterio.mask import mask \n",
    "\n",
    "# --- 1. Define Global Variables (CRITICAL) ---\n",
    "SEQ_LEN = 6      # Number of input time steps (hours)\n",
    "HORIZONS = 3     # Number of prediction time steps (hours)\n",
    "PATCH_H = 13     # Height of the prediction patch\n",
    "PATCH_W = 13     # Width of the prediction patch\n",
    "CHANNELS = 7     # MUST match the model input size\n",
    "THRESHOLD = 0.01 \n",
    "PATCH_SIZE_DEGREES = 0.05 \n",
    "SIMULATED_TARGET_LAT = 34.0522 \n",
    "SIMULATED_TARGET_LON = -118.2437 \n",
    "\n",
    "# --- New Constants for Raster Handling ---\n",
    "DATA_BASE_DIR = r\"C:\\Users\\Ankit\\Datasets_Forest_fire\\Rasters\" # Mock base directory for all GeoTIFFs\n",
    "CLIPPED_OUTPUT_DIR = r\"C:\\Users\\Ankit\\Datasets_Forest_fire\\Clipped_Inputs\" # Directory to save 13x13 NumPy arrays\n",
    "\n",
    "# The 7 actual features used by the model (e.g., T2M replaces ET2M)\n",
    "CHANNELS_LIST = ['t2m', 'd2m', 'tp', 'u10', 'v10', 'lulc', 'DEM'] \n",
    "STATIC_CHANNELS = ['lulc', 'DEM'] # Channels that are time-invariant\n",
    "\n",
    "# Mapping from the internal channel name (for tensor creation) to the full file prefix (from CSV)\n",
    "CHANNEL_FILE_MAP = {\n",
    "    't2m': 'era5_t2m_file', 'd2m': 'era5_d2m_file', 'tp': 'era5_tp_file',\n",
    "    'u10': 'era5_u10_file', 'v10': 'era5_v10_file',\n",
    "    'lulc': 'lulc_file', 'DEM': 'dem_file',\n",
    "}\n",
    "# NOTE: 'viirs_file' is excluded here, assuming it holds the label/ground truth, \n",
    "# not an input feature for the 7-channel model.\n",
    "\n",
    "# --- 2. Define Custom Functions (REQUIRED for Model Loading) ---\n",
    "\n",
    "def slice_output_func(x):\n",
    "    return x[:, :HORIZONS, :, :, :] \n",
    "\n",
    "def squeeze_output_func(x):\n",
    "    return tf.squeeze(x, axis=-1)\n",
    "\n",
    "def slice_output_shape(input_shape):\n",
    "    return (input_shape[0], HORIZONS, input_shape[2], input_shape[3], input_shape[4])\n",
    "\n",
    "def squeeze_output_shape(input_shape):\n",
    "    return input_shape[:-1] \n",
    "\n",
    "CUSTOM_OBJECTS = {\n",
    "    'slice_output_func': slice_output_func, 'slice_output_shape': slice_output_shape,\n",
    "    'squeeze_output_func': squeeze_output_func, 'squeeze_output_shape': squeeze_output_shape,\n",
    "} \n",
    "\n",
    "# --- 3. Define the Date Mapping Function ---\n",
    "\n",
    "def get_sample_date_range(sample_index_in_df):\n",
    "    \"\"\"Calculates the date range for the input and prediction windows.\"\"\"\n",
    "    START_DATE = datetime(2015, 1, 1)\n",
    "\n",
    "    # Input (6-Hour) Window\n",
    "    input_start_index = sample_index_in_df \n",
    "    input_end_index = sample_index_in_df + SEQ_LEN - 1\n",
    "    input_start_time = START_DATE + timedelta(hours=input_start_index)\n",
    "    input_end_time = START_DATE + timedelta(hours=input_end_index)\n",
    "    \n",
    "    # Prediction (3-Hour) Window\n",
    "    pred_start_index = input_end_index + 1\n",
    "    pred_end_index = pred_start_index + HORIZONS - 1\n",
    "    pred_start_time = START_DATE + timedelta(hours=pred_start_index)\n",
    "    pred_end_time = START_DATE + timedelta(hours=pred_end_index)\n",
    "    \n",
    "    return {\n",
    "        'input_start': input_start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'input_end': input_end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'pred_start': pred_start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'pred_end': pred_end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    }\n",
    "\n",
    "# --- 4. Define Geospatial Mapping Function ---\n",
    "\n",
    "def get_geospatial_bounds(lat, lon, size_degrees):\n",
    "    \"\"\"Calculates the latitude and longitude bounds for the 13x13 patch.\"\"\"\n",
    "    half_size = size_degrees / 2\n",
    "    \n",
    "    lat_min = lat - half_size\n",
    "    lat_max = lat + half_size\n",
    "    lon_min = lon - half_size\n",
    "    lon_max = lon + half_size\n",
    "    \n",
    "    return {\n",
    "        'lat_range': (lat_min, lat_max),\n",
    "        'lon_range': (lon_min, lon_max)\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 5. Function to Load and Clip Rasters (UPDATED) ---\n",
    "\n",
    "def load_and_clip_rasters(time_details, geo_details, data_dir, output_dir, channels_list, static_channels):\n",
    "    \"\"\"\n",
    "    Clips the required 13x13 area from the source rasters for the 6 input hours.\n",
    "    Saves the clipped data as NumPy arrays (ready for tensor creation).\n",
    "    Uses CHANNEL_FILE_MAP for correct file naming.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n--- C. Raster Clipping Setup ---\")\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Define the clipping geometry (Bounding Box)\n",
    "    lat_min, lat_max = geo_details['lat_range']\n",
    "    lon_min, lon_max = geo_details['lon_range']\n",
    "    \n",
    "    # GeoJSON format for the clipping polygon (required by rasterio.mask)\n",
    "    geometry = [\n",
    "        {\n",
    "            'type': 'Polygon',\n",
    "            'coordinates': [[\n",
    "                [lon_min, lat_min], [lon_min, lat_max],\n",
    "                [lon_max, lat_max], [lon_max, lat_min],\n",
    "                [lon_min, lat_min]\n",
    "            ]]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 2. Generate the 6 required input hours\n",
    "    start_dt = datetime.strptime(time_details['input_start'], '%Y-%m-%d %H:%M:%S')\n",
    "    end_dt = datetime.strptime(time_details['input_end'], '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    current_dt = start_dt\n",
    "    input_dates = []\n",
    "    while current_dt <= end_dt:\n",
    "        input_dates.append(current_dt)\n",
    "        current_dt += timedelta(hours=1)\n",
    "    \n",
    "    print(f\"Generated {len(input_dates)} input time steps for clipping: {start_dt.strftime('%H:%M')} to {end_dt.strftime('%H:%M')} on {start_dt.strftime('%Y-%m-%d')}.\")\n",
    "    \n",
    "    clipped_arrays = {}\n",
    "\n",
    "    for channel_base_name in channels_list:\n",
    "        clipped_arrays[channel_base_name] = []\n",
    "        # Get the file prefix defined by the user's CSV column names\n",
    "        full_file_prefix = CHANNEL_FILE_MAP[channel_base_name]\n",
    "        \n",
    "        # Determine file paths to load\n",
    "        if channel_base_name in static_channels:\n",
    "            # Static channels only need one file: LULC or DEM\n",
    "            mock_file_name = f\"{full_file_prefix}.tif\"\n",
    "            # Use the first hour's date/time for naming the mock saved file\n",
    "            file_paths = [(Path(data_dir) / full_file_prefix / mock_file_name, input_dates[0])] \n",
    "        else:\n",
    "            # Dynamic channels need 6 sequential files\n",
    "            file_paths = []\n",
    "            for dt in input_dates:\n",
    "                # Assuming dynamic files are named using the full prefix + date_hour\n",
    "                mock_file_name = f\"{full_file_prefix}_{dt.strftime('%Y%m%d_%H')}.tif\"\n",
    "                file_paths.append((Path(data_dir) / full_file_prefix / mock_file_name, dt))\n",
    "        \n",
    "        for file_path, dt in file_paths:\n",
    "            print(f\"  - Processing: {file_path.name}\")\n",
    "            \n",
    "            # --- MOCK CLIPPING LOGIC ---\n",
    "            try:\n",
    "                # Simulate the clipped array result: (1, 13, 13)\n",
    "                # Value is set to channel index for easy identification in the final tensor\n",
    "                channel_index = channels_list.index(channel_base_name)\n",
    "                clipped_data = np.full((1, PATCH_H, PATCH_W), fill_value=channel_index + 1, dtype=np.float32)\n",
    "                \n",
    "                # Save the clipped NumPy array\n",
    "                output_file_name = f\"clipped_{channel_base_name}_{dt.strftime('%Y%m%d_%H')}.npy\"\n",
    "                np.save(output_path / output_file_name, clipped_data)\n",
    "                \n",
    "                clipped_arrays[channel_base_name].append(clipped_data.squeeze())\n",
    "                print(f\"    -> MOCK SUCCESS: Clipped and saved 13x13 array to {output_file_name}\")\n",
    "\n",
    "                # --- REAL RASTERIO LOGIC (UNCOMMENT AND USE LOCALLY) ---\n",
    "                # with rasterio.open(file_path) as src:\n",
    "                #     out_image, out_transform = mask(src, geometry, crop=True)\n",
    "                #     # Save the clipped NumPy array\n",
    "                #     output_file_name = f\"clipped_{channel_base_name}_{dt.strftime('%Y%m%d_%H')}.npy\"\n",
    "                #     np.save(output_path / output_file_name, out_image)\n",
    "                #     clipped_arrays[channel_base_name].append(out_image.squeeze())\n",
    "                #     print(f\"    -> REAL SUCCESS: Saved clipped 13x13 array to {output_file_name}\")\n",
    "                \n",
    "                # If static, the first clipped array is duplicated 6 times for SEQ_LEN\n",
    "                if channel_base_name in static_channels:\n",
    "                    static_array = clipped_arrays[channel_base_name][0]\n",
    "                    for _ in range(SEQ_LEN - 1):\n",
    "                         clipped_arrays[channel_base_name].append(static_array)\n",
    "                    print(f\"    -> STATIC: Repeated array {SEQ_LEN} times for {channel_base_name}.\")\n",
    "                    break # Exit the inner loop since the sequence is full\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    -> ERROR: Could not process {file_path.name}. Check if the GeoTIFF file exists and is valid. Error: {e}\")\n",
    "                clipped_arrays[channel_base_name].append(np.full((PATCH_H, PATCH_W), np.nan)) # Use NaNs as placeholder\n",
    "                \n",
    "    # 3. Create the final tensor (SEQ_LEN, PATCH_H, PATCH_W, CHANNELS)\n",
    "    all_channels_stacked = [np.stack(clipped_arrays[c], axis=0) for c in channels_list] \n",
    "    \n",
    "    if not all(arr.shape[0] == SEQ_LEN for arr in all_channels_stacked):\n",
    "        print(\"\\n[ERROR] Final tensor construction failed: Not all channels have the required SEQ_LEN. Check file integrity.\")\n",
    "        return None\n",
    "    \n",
    "    # Stack the sequence arrays along the channel axis (-1)\n",
    "    final_input_tensor = np.stack(all_channels_stacked, axis=-1)\n",
    "    \n",
    "    if final_input_tensor.shape == (SEQ_LEN, PATCH_H, PATCH_W, CHANNELS):\n",
    "        print(f\"\\n✅ FINAL INPUT TENSOR CREATED: Shape {final_input_tensor.shape}. It is ready for model inference.\")\n",
    "        return final_input_tensor\n",
    "    else:\n",
    "        print(f\"\\n[ERROR] Final tensor has incorrect shape: {final_input_tensor.shape}. Expected ({SEQ_LEN}, {PATCH_H}, {PATCH_W}, {CHANNELS}).\")\n",
    "        return None\n",
    "\n",
    "# --- 6. Main Execution Setup (Loading Data/Model) ---\n",
    "\n",
    "MODEL_PATH = r\"C:\\Users\\Ankit\\Downloads\\final_model.h5\" \n",
    "csv_path = r\"C:\\Users\\Ankit\\Datasets_Forest_fire\\sequence_index_hourly_binary.csv\"\n",
    "\n",
    "# Load the full DataFrame\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.reset_index(drop=True) \n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: CSV file not found at {csv_path}. Cannot map dates.\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading CSV: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "TOTAL = len(df)\n",
    "VAL_SPLIT = 0.2\n",
    "val_size = int(TOTAL * VAL_SPLIT)\n",
    "val_df = df.iloc[:val_size].copy() \n",
    "\n",
    "SUCCESS_SAMPLE_INDEX_IN_VAL_DF = 17 \n",
    "\n",
    "try:\n",
    "    print(f\"Loading model from: {MODEL_PATH}\")\n",
    "    model = tf.keras.models.load_model(\n",
    "        MODEL_PATH, \n",
    "        custom_objects=CUSTOM_OBJECTS,\n",
    "        safe_mode=False \n",
    "    )\n",
    "    print(\"Model loaded successfully.\")\n",
    "    \n",
    "    # --- 7. Execute Analysis and Mapping ---\n",
    "    \n",
    "    original_df_index = val_df.index[SUCCESS_SAMPLE_INDEX_IN_VAL_DF]\n",
    "    time_details = get_sample_date_range(original_df_index)\n",
    "    geo_details = get_geospatial_bounds(\n",
    "        SIMULATED_TARGET_LAT, \n",
    "        SIMULATED_TARGET_LON, \n",
    "        PATCH_SIZE_DEGREES\n",
    "    )\n",
    "\n",
    "    # --- 8. Execute Clipping and Tensor Creation (NEW STEP) ---\n",
    "    input_tensor = load_and_clip_rasters(\n",
    "        time_details, \n",
    "        geo_details, \n",
    "        DATA_BASE_DIR, \n",
    "        CLIPPED_OUTPUT_DIR, \n",
    "        CHANNELS_LIST, \n",
    "        STATIC_CHANNELS\n",
    "    )\n",
    "    \n",
    "    # --- 9. Print Final Output ---\n",
    "    \n",
    "    print(\"\\n=======================================================\")\n",
    "    print(\"✅ **SUCCESSFUL FIRE PREDICTION ANALYSIS**\")\n",
    "    print(\"=======================================================\")\n",
    "    print(f\"Validation Sample Index (in Val DF): {SUCCESS_SAMPLE_INDEX_IN_VAL_DF}\")\n",
    "    print(f\"Original Data Row Index (used for offset): {original_df_index}\")\n",
    "    print(f\"Prediction Threshold Used: {THRESHOLD:.2f}\")\n",
    "\n",
    "    print(\"\\n--- A. Time Windows ---\")\n",
    "    print(f\"Input (6-Hour) Window:\")\n",
    "    print(f\"  Start Time: {time_details['input_start']}\")\n",
    "    print(f\"  End Time:   {time_details['input_end']}\")\n",
    "    \n",
    "    print(f\"\\nPrediction (3-Hour) Output Window:\")\n",
    "    print(f\"  Start Time: {time_details['pred_start']}\")\n",
    "    print(f\"  End Time:   {time_details['pred_end']}\")\n",
    "    \n",
    "    print(\"\\n--- B. Geospatial Patch (13x13 Pixels) ---\")\n",
    "    print(f\"Center Coordinate (Simulated): Lat {SIMULATED_TARGET_LAT}, Lon {SIMULATED_TARGET_LON}\")\n",
    "    print(f\"Approximate Patch Size: {PATCH_SIZE_DEGREES:.4f} degrees x {PATCH_SIZE_DEGREES:.4f} degrees\")\n",
    "    print(f\"Latitude Range:                {geo_details['lat_range'][0]:.4f} to {geo_details['lat_range'][1]:.4f} degrees\")\n",
    "    print(f\"Longitude Range:               {geo_details['lon_range'][0]:.4f} to {geo_details['lon_range'][1]:.4f} degrees\")\n",
    "    print(f\"\\n--- D. Prediction Inference ---\")\n",
    "    if input_tensor is not None:\n",
    "        # NOTE: You can now run the prediction using your loaded model!\n",
    "        # prediction = model.predict(np.expand_dims(input_tensor, axis=0))\n",
    "        # print(f\"Model prediction successfully simulated with input shape {input_tensor.shape}.\")\n",
    "        print(f\"Input tensor successfully created (Shape: {input_tensor.shape}). Ready for model.predict().\")\n",
    "    else:\n",
    "        print(\"Input tensor could not be created due to file processing errors.\")\n",
    "    print(\"=======================================================\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[CRITICAL ERROR] Failed during model load or execution. Error: {e}\")\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417656c8-fe50-4766-9fa6-6c16cd748d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
